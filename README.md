<div align="center"><div align="center"><div align="center"># WenKB - æ™ºèƒ½ä¸ªäººçŸ¥è¯†åº“æ¡Œé¢åº”ç”¨



# ğŸ“š KAI - Personal Local RAG



**Next-Generation Knowledge Management Tool Powered by LLM**# ğŸ“š KAI - Personal Local RAG



[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)

[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)**Next-Generation Knowledge Management Tool Powered by LLM**# ğŸ“š KAI - æ™ºèƒ½ä¸ªäººçŸ¥è¯†åº“WenKB æ˜¯ä¸€æ¬¾åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸‹ä¸€ä»£çŸ¥è¯†ç®¡ç†å·¥å…·ï¼Œèåˆ AI èƒ½åŠ›é‡æ–°å®šä¹‰çŸ¥è¯†ç»„ç»‡æ–¹å¼ã€‚æ”¯æŒå¤šæºçŸ¥è¯†æ•´åˆã€æ™ºèƒ½é—®ç­”ã€è‡ªåŠ¨åŒ–çŸ¥è¯†åŠ å·¥å’Œå¯è§†åŒ–çŸ¥è¯†ç½‘ç»œï¼ŒåŠ©åŠ›æ„å»ºæ‚¨çš„ç¬¬äºŒå¤§è„‘ã€‚

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)



[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)

</div>

[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

---

[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)**åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸‹ä¸€ä»£çŸ¥è¯†ç®¡ç†å·¥å…·**## âœ¨ æ ¸å¿ƒåŠŸèƒ½

## âœ¨ Features

[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

### ğŸ“– Smart Knowledge Base Management

- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more

- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction

- **Document Management** - Version control, knowledge linking, batch operations[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)

- **Dual-mode Editor** - Markdown + Rich text hybrid editing

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)### æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†

### ğŸ” Intelligent Search & Q&A

- **Semantic Search** - Deep cross-document retrieval based on vector database</div>

- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning

- **Local Deployment** - Fully local data storage, privacy protection[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)- **å¤šæ ¼å¼å¯¼å…¥**ï¼šæ”¯æŒæ–‡æ¡£ï¼ˆPDF/Word/Markdownï¼‰ã€ç½‘é¡µé“¾æ¥ã€çº¯æ–‡æœ¬ç­‰å¤šæºæ•°æ®æ¥å…¥



### ğŸ¤– AI Capabilities---

- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, Mistral and more

- **Chinese Optimized** - Built-in M3E Chinese embedding model for better semantic understanding[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)- **AI è‡ªåŠ¨åŒ–å¤„ç†**ï¼šè‡ªåŠ¨åˆ†æ®µã€ç”Ÿæˆæ‘˜è¦ã€åˆ›å»º Q&A å¯¹ã€æå–çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„



---## âœ¨ Features



## ğŸš€ Quick Start[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)- **åŠ¨æ€ç»´æŠ¤**ï¼šæ”¯æŒæ–‡æ¡£ç‰ˆæœ¬ç®¡ç†ã€çŸ¥è¯†å…³è”æ ‡æ³¨ã€æ‰¹é‡å¤„ç†æ“ä½œ



### Requirements### ğŸ“– Smart Knowledge Base Management



| Dependency | Version | Description |- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more- **åŒæ¨¡å¼ç¼–è¾‘å™¨**ï¼šMarkdown + å¯Œæ–‡æœ¬æ··åˆç¼–è¾‘

|------------|---------|-------------|

| Python | >= 3.11 | Backend runtime |- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction

| Node.js | >= 18.x | Frontend runtime |

| Ollama | Latest | Local LLM service |- **Document Management** - Version control, knowledge linking, batch operations[åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹) â€¢ [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„) â€¢ [é…ç½®è¯´æ˜](#-é…ç½®è¯´æ˜) â€¢ [å¼€å‘æŒ‡å—](#-å¼€å‘æŒ‡å—)



### 1. Clone the Repository- **Dual-mode Editor** - Markdown + Rich text hybrid editing



```bash### å¼ºå¤§çš„æœç´¢ä¸é—®ç­”

git clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git

cd Kai-personalLocalRAG### ğŸ” Intelligent Search & Q&A

```

- **Semantic Search** - Deep cross-document retrieval based on vector database</div>- **è¯­ä¹‰æœç´¢**ï¼šåŸºäºå‘é‡æ•°æ®åº“çš„è·¨æ–‡æ¡£æ·±åº¦æ£€ç´¢

### 2. Install Ollama

- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning

```bash

# macOS- **Local Deployment** - Fully local data storage, privacy protection- **å¯¹è¯å¼äº¤äº’**ï¼šæ”¯æŒè¿½é—®ã€æº¯æºå¼•ç”¨ã€å¤šè½®çŸ¥è¯†æ¨ç†

brew install ollama



# Linux

curl -fsSL https://ollama.com/install.sh | sh### ğŸ¤– AI Capabilities---



# Start service and download model- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, Mistral and more

ollama serve

ollama pull qwen2- **Chinese Optimized** - Built-in M3E Chinese embedding model for better semantic understanding---

```



### 3. One-Click Start

---## âœ¨ åŠŸèƒ½ç‰¹æ€§

```bash

# Add execute permission

chmod +x start.sh stop.sh

## ğŸš€ Quick Start## ğŸ—ï¸ é¡¹ç›®æ¶æ„

# Start all services

./start.sh

```

### Requirements### ğŸ“– æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†

### 4. Access the Application



- **Frontend**: http://localhost:11420

- **Backend API**: http://localhost:6088| Dependency | Version | Description |- **å¤šæ ¼å¼å¯¼å…¥** - æ”¯æŒ PDFã€Wordã€Markdownã€ç½‘é¡µé“¾æ¥ã€çº¯æ–‡æœ¬ç­‰å¤šæºæ•°æ®```

- **API Docs**: http://localhost:6088/docs

|------------|---------|-------------|

### 5. Stop Services

| Python | >= 3.11 | Backend runtime |- **AI è‡ªåŠ¨å¤„ç†** - è‡ªåŠ¨åˆ†æ®µã€ç”Ÿæˆæ‘˜è¦ã€åˆ›å»º Q&A å¯¹ã€æå–çŸ¥è¯†è¦ç‚¹kai-main/

```bash

./stop.sh| Node.js | >= 18.x | Frontend runtime |

```

| Ollama | Latest | Local LLM service |- **æ–‡æ¡£ç®¡ç†** - æ”¯æŒç‰ˆæœ¬ç®¡ç†ã€çŸ¥è¯†å…³è”ã€æ‰¹é‡æ“ä½œâ”œâ”€â”€ wenkb-client/          # å‰ç«¯ (Vue 3 + Vite + Tauri)

---



## ğŸ“ Project Structure

### 1. Clone the Repository- **åŒæ¨¡å¼ç¼–è¾‘** - Markdown + å¯Œæ–‡æœ¬æ··åˆç¼–è¾‘å™¨â”‚   â”œâ”€â”€ src/

```

kai-main/

â”œâ”€â”€ kai-client/              # Frontend (Vue 3 + Vite + Tauri)

â”‚   â”œâ”€â”€ src/```bashâ”‚   â”‚   â”œâ”€â”€ views/         # é¡µé¢ç»„ä»¶

â”‚   â”‚   â”œâ”€â”€ views/           # Page components

â”‚   â”‚   â”œâ”€â”€ components/      # Reusable componentsgit clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git

â”‚   â”‚   â”œâ”€â”€ store/           # State management

â”‚   â”‚   â”œâ”€â”€ libs/            # Utility librariescd Kai-personalLocalRAG### ğŸ” æ™ºèƒ½æœç´¢ä¸é—®ç­”â”‚   â”‚   â”œâ”€â”€ components/    # é€šç”¨ç»„ä»¶

â”‚   â”‚   â””â”€â”€ router/          # Route configuration

â”‚   â””â”€â”€ src-tauri/           # Tauri desktop app config```

â”‚

â”œâ”€â”€ kai-server/              # Backend (Python FastAPI)- **è¯­ä¹‰æœç´¢** - åŸºäºå‘é‡æ•°æ®åº“çš„è·¨æ–‡æ¡£æ·±åº¦æ£€ç´¢â”‚   â”‚   â”œâ”€â”€ store/         # çŠ¶æ€ç®¡ç†

â”‚   â”œâ”€â”€ server/

â”‚   â”‚   â”œâ”€â”€ api/             # API endpoints### 2. Install Ollama

â”‚   â”‚   â”œâ”€â”€ core/            # Core business logic

â”‚   â”‚   â”œâ”€â”€ db/              # Database operations- **å¯¹è¯å¼é—®ç­”** - æ”¯æŒè¿½é—®ã€æº¯æºå¼•ç”¨ã€å¤šè½®çŸ¥è¯†æ¨ç†â”‚   â”‚   â””â”€â”€ router/        # è·¯ç”±é…ç½®

â”‚   â”‚   â””â”€â”€ model/           # Data models

â”‚   â”œâ”€â”€ config/              # Configuration files```bash

â”‚   â””â”€â”€ resources/           # Resource files

â”‚# macOS- **æœ¬åœ°éƒ¨ç½²** - æ•°æ®å®Œå…¨æœ¬åœ°åŒ–ï¼Œä¿æŠ¤éšç§å®‰å…¨â”‚   â””â”€â”€ src-tauri/         # Tauri æ¡Œé¢åº”ç”¨é…ç½®

â”œâ”€â”€ start.sh                 # Startup script

â”œâ”€â”€ stop.sh                  # Shutdown scriptbrew install ollama

â””â”€â”€ README.md

```â”‚



---# Linux



## âš™ï¸ Configurationcurl -fsSL https://ollama.com/install.sh | sh### ğŸ¤– AI èƒ½åŠ›â”œâ”€â”€ wenkb-server/          # åç«¯ (Python FastAPI)



### LLM Configuration



Edit `kai-server/config/llm.py`:# Start service and download model- **æœ¬åœ° LLM** - é›†æˆ Ollamaï¼Œæ”¯æŒ Qwenã€Llamaã€Mistral ç­‰æ¨¡å‹â”‚   â”œâ”€â”€ server/



```pythonollama serve

# Ollama service address

OLLAMA_HOST = "http://localhost:11434"ollama pull qwen2- **ä¸­æ–‡ä¼˜åŒ–** - å†…ç½® M3E ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œè¯­ä¹‰ç†è§£æ›´ç²¾å‡†â”‚   â”‚   â”œâ”€â”€ api/           # API æ¥å£



# Default LLM model```

DEFAULT_LLM_MODEL = "qwen2"

â”‚   â”‚   â”œâ”€â”€ core/          # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

# Embedding model path

EMBEDDING_MODEL_PATH = "./resources/model/m3e"### 3. One-Click Start

```

---â”‚   â”‚   â”œâ”€â”€ db/            # æ•°æ®åº“æ“ä½œ

### Database Configuration

```bash

Edit `kai-server/config/datasource.py`:

# Add execute permissionâ”‚   â”‚   â””â”€â”€ model/         # æ•°æ®æ¨¡å‹

```python

# SQLite database pathchmod +x start.sh stop.sh

DATABASE_PATH = "./resources/database/kai.db"

## ğŸš€ å¿«é€Ÿå¼€å§‹â”‚   â”œâ”€â”€ config/            # é…ç½®æ–‡ä»¶

# Vector store path

VECTOR_STORE_PATH = "./resources/vector_store"# Start all services

```

./start.shâ”‚   â””â”€â”€ resources/         # èµ„æºæ–‡ä»¶

### Frontend Configuration



Edit `kai-client/vite.config.js`:

# Check service status### ç¯å¢ƒè¦æ±‚â”‚       â”œâ”€â”€ database/      # SQLite æ•°æ®åº“

```javascript

// API proxy configuration./start.sh status

proxy: {

  '/api': {â”‚       â””â”€â”€ model/         # æœ¬åœ°åµŒå…¥æ¨¡å‹ (m3e)

    target: 'http://localhost:6088',

    changeOrigin: true# Stop services

  }

}./stop.sh| ä¾èµ– | ç‰ˆæœ¬ | è¯´æ˜ |â”‚

```

```

---

|------|------|------|â”œâ”€â”€ start.sh               # ä¸€é”®å¯åŠ¨è„šæœ¬

## ğŸ› ï¸ Development

### 4. Access the Application

### Backend Development

| Python | >= 3.11 | åç«¯è¿è¡Œç¯å¢ƒ |â””â”€â”€ stop.sh                # åœæ­¢è„šæœ¬

```bash

cd kai-serverAfter successful startup, visit: **http://localhost:11420**



# Create virtual environment| Node.js | >= 18.x | å‰ç«¯è¿è¡Œç¯å¢ƒ |```

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activate| Service | Port | Description |



# Install dependencies|---------|------|-------------|| Ollama | æœ€æ–°ç‰ˆ | æœ¬åœ° LLM æœåŠ¡ |

pip install -r requirements.txt

| Frontend | 11420 | Web interface |

# Start development server

python app.py| Backend | 6088 | API service |---

```

| Ollama | 11434 | LLM service |

### Frontend Development

### 1. å…‹éš†é¡¹ç›®

```bash

cd kai-client---



# Install dependencies## ğŸš€ å¿«é€Ÿå¼€å§‹

npm install

## ğŸ“ Project Structure

# Start development server

npm run dev```bash



# Build for production```

npm run build

```kai/git clone https://github.com/yourusername/kai.git### ç¯å¢ƒè¦æ±‚



### Desktop App (Tauri)â”œâ”€â”€ kai-client/                # Frontend project



```bashâ”‚   â”œâ”€â”€ src/cd kai

cd kai-client

â”‚   â”‚   â”œâ”€â”€ views/            # Page components

# Install Tauri CLI

npm install -g @tauri-apps/cliâ”‚   â”‚   â”œâ”€â”€ components/       # Common components```- **Node.js** >= 18.x



# Development modeâ”‚   â”‚   â”œâ”€â”€ store/            # Pinia state management

npm run tauri dev

â”‚   â”‚   â”œâ”€â”€ router/           # Vue Router- **Python** >= 3.11

# Build desktop app

npm run tauri buildâ”‚   â”‚   â””â”€â”€ config/           # Frontend config

```

â”‚   â”œâ”€â”€ src-tauri/            # Tauri desktop app config### 2. å®‰è£… Ollama- **Ollama** (æœ¬åœ° LLM æœåŠ¡)

---

â”‚   â”œâ”€â”€ package.json

## ğŸ“‹ Tech Stack

â”‚   â””â”€â”€ vite.config.js

### Backend

- **Framework**: FastAPIâ”‚

- **Database**: SQLite + SQLAlchemy

- **Vector Store**: ChromaDBâ”œâ”€â”€ kai-server/                # Backend project```bash### ä¸€é”®å¯åŠ¨

- **LLM**: Ollama (Qwen2, Llama, etc.)

- **Embedding**: M3E (Local model)â”‚   â”œâ”€â”€ server/

- **Document Processing**: LangChain

â”‚   â”‚   â”œâ”€â”€ api/              # API layer# macOS

### Frontend

- **Framework**: Vue 3 + Composition APIâ”‚   â”‚   â”œâ”€â”€ core/             # Core business logic

- **Build Tool**: Vite 5.x

- **UI Library**: Naive UIâ”‚   â”‚   â”œâ”€â”€ db/               # Database operationsbrew install ollama```bash

- **Desktop**: Tauri 2.0

- **State Management**: Piniaâ”‚   â”‚   â”œâ”€â”€ model/            # Data models



---â”‚   â”‚   â””â”€â”€ utils/            # Utility functionscd kai-main



## â“ FAQâ”‚   â”œâ”€â”€ config/               # Configuration files



### Q: How to change the LLM model?â”‚   â”œâ”€â”€ resources/            # Resource directory# Linux

A: Modify `DEFAULT_LLM_MODEL` in `kai-server/config/llm.py`, or configure through the web interface in Settings.

â”‚   â”‚   â”œâ”€â”€ database/         # SQLite database

### Q: Why is semantic search slow?

A: First search requires loading the embedding model. Subsequent searches will be faster.â”‚   â”‚   â”œâ”€â”€ documents/        # Uploaded documentscurl -fsSL https://ollama.com/install.sh | sh# å¯åŠ¨æ‰€æœ‰æœåŠ¡ (Ollama + åç«¯ + å‰ç«¯)



### Q: How to import large documents?â”‚   â”‚   â”œâ”€â”€ model/            # Embedding model

A: The system automatically chunks large documents. You can adjust chunk size in configuration.

â”‚   â”‚   â””â”€â”€ vector_store/     # Vector storage./start.sh

### Q: Does it support GPU acceleration?

A: Yes, if Ollama is configured with GPU support, LLM inference will be accelerated automatically.â”‚   â”œâ”€â”€ app.py                # Application entry



---â”‚   â””â”€â”€ requirements.txt# å¯åŠ¨æœåŠ¡å¹¶ä¸‹è½½æ¨¡å‹



## ğŸ¤ Contributingâ”‚



Contributions are welcome! Please feel free to submit a Pull Request.â”œâ”€â”€ start.sh                   # One-click start scriptollama serve# åœæ­¢æ‰€æœ‰æœåŠ¡



1. Fork the repositoryâ”œâ”€â”€ stop.sh                    # Stop script

2. Create your feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)â””â”€â”€ README.mdollama pull qwen2./stop.sh

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request```



---```



## ğŸ“„ License---



This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.# é‡å¯æœåŠ¡



---## âš™ï¸ Configuration



<div align="center">### 3. ä¸€é”®å¯åŠ¨./start.sh restart



**Built with â¤ï¸ by [kaiyu-li317](https://github.com/kaiyu-li317)**### Backend Configuration



</div>


Configuration files are located in `kai-server/config/`:

```bash# æŸ¥çœ‹æœåŠ¡çŠ¶æ€

| File | Description |

|------|-------------|# æ·»åŠ æ‰§è¡Œæƒé™./start.sh status

| `llm.py` | LLM model config (Ollama address, model name) |

| `common.py` | Common config (port, paths, etc.) |chmod +x start.sh stop.sh```

| `datasource.py` | Data source config (database connection) |



### Embedding Model

# å¯åŠ¨æ‰€æœ‰æœåŠ¡### æœåŠ¡ç«¯å£

The project uses **M3E** Chinese embedding model, which will be automatically downloaded to `kai-server/resources/model/m3e/` on first startup.

./start.sh

| Model | Parameters | Vector Dim | Features |

|-------|------------|------------|----------|| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |

| m3e-small | 24M | 512 | Lightweight, Chinese optimized |

| m3e-base | 110M | 768 | Higher accuracy, bilingual |# æŸ¥çœ‹æœåŠ¡çŠ¶æ€|------|------|------|



### LLM Models./start.sh status| å‰ç«¯ | 11420 | Vue 3 å¼€å‘æœåŠ¡å™¨ |



Recommended Ollama models:| åç«¯ | 6088 | FastAPI æœåŠ¡ |



```bash# åœæ­¢æœåŠ¡| Ollama | 11434 | æœ¬åœ° LLM æœåŠ¡ |

ollama pull qwen2        # Recommended for Chinese

ollama pull llama3.1     # Best for English./stop.sh

ollama pull mistral      # Balanced choice

``````è®¿é—®åœ°å€ï¼šhttp://localhost:11420



---



## ğŸ› ï¸ Development### 4. è®¿é—®åº”ç”¨---



### Manual Start



**Backend:**å¯åŠ¨æˆåŠŸåè®¿é—®ï¼š**http://localhost:11420**## ğŸ”§ æ‰‹åŠ¨å®‰è£…



```bash

cd kai-server

| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |### 1. å®‰è£… Ollama

# Create virtual environment

python -m venv .venv|------|------|------|

source .venv/bin/activate  # Windows: .venv\Scripts\activate

| å‰ç«¯ | 11420 | Web ç•Œé¢ |```bash

# Install dependencies

pip install -r requirements.txt| åç«¯ | 6088 | API æœåŠ¡ |# macOS



# Start service| Ollama | 11434 | LLM æœåŠ¡ |brew install ollama

python app.py

```



**Frontend:**---# å¯åŠ¨æœåŠ¡å¹¶ä¸‹è½½æ¨¡å‹



```bashollama serve

cd kai-client

## ğŸ“ é¡¹ç›®ç»“æ„ollama pull qwen2

# Install dependencies

npm install```



# Start dev server```

npm run dev

kai/### 2. åç«¯æœåŠ¡

# Build for production

npm run buildâ”œâ”€â”€ kai-client/                # å‰ç«¯é¡¹ç›®

```

â”‚   â”œâ”€â”€ src/```bash

### Tech Stack

â”‚   â”‚   â”œâ”€â”€ views/            # é¡µé¢ç»„ä»¶cd wenkb-server

**Frontend:**

- Vue 3 + Composition APIâ”‚   â”‚   â”œâ”€â”€ components/       # é€šç”¨ç»„ä»¶

- Vite 5.x

- Naive UIâ”‚   â”‚   â”œâ”€â”€ store/            # Pinia çŠ¶æ€ç®¡ç†# å®‰è£…ä¾èµ–

- Pinia

- Tauri 2.0 (optional, for desktop app)â”‚   â”‚   â”œâ”€â”€ router/           # Vue Router è·¯ç”±pip install -r requirements.txt



**Backend:**â”‚   â”‚   â””â”€â”€ config/           # å‰ç«¯é…ç½®

- Python 3.11+

- FastAPIâ”‚   â”œâ”€â”€ src-tauri/            # Tauri æ¡Œé¢åº”ç”¨é…ç½®# å¯åŠ¨æœåŠ¡

- SQLAlchemy + SQLite

- LangChainâ”‚   â”œâ”€â”€ package.jsonpython app.py

- ChromaDB

â”‚   â””â”€â”€ vite.config.js# æˆ–

---

â”‚uvicorn app:app --host 0.0.0.0 --port 6088 --reload

## ğŸ“ FAQ

â”œâ”€â”€ kai-server/                # åç«¯é¡¹ç›®```

<details>

<summary><b>Q: Port already in use?</b></summary>â”‚   â”œâ”€â”€ server/



```bashâ”‚   â”‚   â”œâ”€â”€ api/              # API æ¥å£å±‚### 3. å‰ç«¯æœåŠ¡

# Find process using the port

lsof -i :6088â”‚   â”‚   â”œâ”€â”€ core/             # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

lsof -i :11420

â”‚   â”‚   â”œâ”€â”€ db/               # æ•°æ®åº“æ“ä½œ```bash

# Kill the process

kill -9 <PID>â”‚   â”‚   â”œâ”€â”€ model/            # æ•°æ®æ¨¡å‹cd wenkb-client

```

</details>â”‚   â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°



<details>â”‚   â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶# å®‰è£…ä¾èµ–

<summary><b>Q: Ollama model download is slow?</b></summary>

â”‚   â”œâ”€â”€ resources/            # èµ„æºç›®å½•npm install

You can set up a proxy or use a mirror:

```bashâ”‚   â”‚   â”œâ”€â”€ database/         # SQLite æ•°æ®åº“

export OLLAMA_HOST=https://your-mirror.com

ollama pull qwen2â”‚   â”‚   â”œâ”€â”€ documents/        # ä¸Šä¼ æ–‡æ¡£# å¯åŠ¨å¼€å‘æœåŠ¡å™¨

```

</details>â”‚   â”‚   â”œâ”€â”€ model/            # åµŒå…¥æ¨¡å‹npm run dev



<details>â”‚   â”‚   â””â”€â”€ vector_store/     # å‘é‡å­˜å‚¨```

<summary><b>Q: M3E model loading failed?</b></summary>

â”‚   â”œâ”€â”€ app.py                # åº”ç”¨å…¥å£

Check your network connection. The model will be automatically downloaded from HuggingFace. If there are network issues, you can manually download it to the `kai-server/resources/model/m3e/` directory.

</details>â”‚   â””â”€â”€ requirements.txt---



---â”‚



## ğŸ¤ Contributingâ”œâ”€â”€ start.sh                   # ä¸€é”®å¯åŠ¨è„šæœ¬## ğŸ¤– AI æ¨¡å‹é…ç½®



Contributions are welcome! Feel free to submit Issues and Pull Requests.â”œâ”€â”€ stop.sh                    # åœæ­¢è„šæœ¬



1. Fork this repositoryâ””â”€â”€ README.md### LLM æ¨¡å‹ (Ollama)

2. Create a feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)```

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Create a Pull Requestæœ¬é¡¹ç›®ä½¿ç”¨ Ollama ä½œä¸ºæœ¬åœ° LLM æœåŠ¡ï¼Œé»˜è®¤ä½¿ç”¨ `qwen2` æ¨¡å‹ã€‚



------



## ğŸ“„ License```bash



This project is licensed under the [MIT License](LICENSE).## âš™ï¸ é…ç½®è¯´æ˜# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹



---ollama list



## ğŸ™ Acknowledgements### åç«¯é…ç½®



- [Ollama](https://ollama.ai/) - Local LLM service# å®‰è£…å…¶ä»–æ¨¡å‹

- [M3E](https://huggingface.co/moka-ai/m3e-small) - Chinese embedding model

- [LangChain](https://langchain.com/) - LLM application frameworké…ç½®æ–‡ä»¶ä½äº `kai-server/config/` ç›®å½•ï¼šollama pull llama3.1

- [Naive UI](https://naiveui.com/) - Vue 3 component library

- [FastAPI](https://fastapi.tiangolo.com/) - Python web frameworkollama pull mistral


| æ–‡ä»¶ | è¯´æ˜ |```

|------|------|

| `llm.py` | LLM æ¨¡å‹é…ç½® (Ollama åœ°å€ã€æ¨¡å‹åç§°) |### åµŒå…¥æ¨¡å‹ (M3E)

| `common.py` | é€šç”¨é…ç½® (ç«¯å£ã€è·¯å¾„ç­‰) |

| `datasource.py` | æ•°æ®æºé…ç½® (æ•°æ®åº“è¿æ¥) |é¡¹ç›®å†…ç½® `m3e-small` ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œç”¨äºæ–‡æ¡£å‘é‡åŒ–å’Œè¯­ä¹‰æœç´¢ã€‚



### åµŒå…¥æ¨¡å‹**M3E æ¨¡å‹ç‰¹ç‚¹ï¼š**

- æ”¯æŒä¸­è‹±åŒè¯­

é¡¹ç›®ä½¿ç”¨ **M3E** ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œé¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `kai-server/resources/model/m3e/`ã€‚- åƒä¸‡çº§ä¸­æ–‡å¥å¯¹æ•°æ®é›†è®­ç»ƒ

- 512 ç»´å‘é‡è¾“å‡º

| æ¨¡å‹ | å‚æ•°é‡ | å‘é‡ç»´åº¦ | ç‰¹ç‚¹ |- é€‚ç”¨äºæ–‡æœ¬ç›¸ä¼¼åº¦ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡

|------|--------|----------|------|

| m3e-small | 24M | 512 | è½»é‡å¿«é€Ÿï¼Œä¸­æ–‡ä¼˜åŒ– || æ¨¡å‹ | å‚æ•°é‡ | ç»´åº¦ | ä¸­æ–‡ | è‹±æ–‡ |

| m3e-base | 110M | 768 | ç²¾åº¦æ›´é«˜ï¼Œä¸­è‹±åŒè¯­ ||------|--------|------|------|------|

| m3e-small | 24M | 512 | âœ… | âŒ |

### LLM æ¨¡å‹| m3e-base | 110M | 768 | âœ… | âœ… |



æ¨èä½¿ç”¨çš„ Ollama æ¨¡å‹ï¼š---



```bash## ğŸ“ é…ç½®æ–‡ä»¶è¯´æ˜

ollama pull qwen2        # æ¨èï¼Œä¸­æ–‡æ•ˆæœæœ€ä½³

ollama pull llama3.1     # è‹±æ–‡æ•ˆæœå¥½### åç«¯é…ç½®

ollama pull mistral      # å¹³è¡¡é€‰æ‹©

```- `config/llm.py` - LLM æ¨¡å‹é…ç½® (Ollama)

- `config/common.py` - é€šç”¨é…ç½®

---- `config/datasource.py` - æ•°æ®æºé…ç½®



## ğŸ› ï¸ å¼€å‘æŒ‡å—### å‰ç«¯é…ç½®



### æ‰‹åŠ¨å¯åŠ¨- `vite.config.js` - Vite æ„å»ºé…ç½®

- `src/config/index.ts` - å‰ç«¯é…ç½®

**åç«¯ï¼š**

---

```bash

cd kai-server## ğŸ› ï¸ å¼€å‘è¯´æ˜



# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ### å‰ç«¯æŠ€æœ¯æ ˆ

python -m venv .venv

source .venv/bin/activate  # Windows: .venv\Scripts\activate- Vue 3 + Composition API

- Vite 5.x

# å®‰è£…ä¾èµ–- Naive UI ç»„ä»¶åº“

pip install -r requirements.txt- Tauri 2.0 (æ¡Œé¢åº”ç”¨)

- Pinia çŠ¶æ€ç®¡ç†

# å¯åŠ¨æœåŠ¡

python app.py### åç«¯æŠ€æœ¯æ ˆ

```

- Python 3.11+

**å‰ç«¯ï¼š**- FastAPI

- SQLAlchemy + SQLite

```bash- LangChain + Ollama

cd kai-client- ChromaDB (å‘é‡æ•°æ®åº“)



# å®‰è£…ä¾èµ–### åº”ç”¨æµç¨‹èŠ‚ç‚¹ (å¾…å¼€å‘)

npm install

> å‚è€ƒ Dify/FastGPT çš„å·¥ä½œæµè®¾è®¡

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨

npm run dev**Start èŠ‚ç‚¹è®¾è®¡è€ƒè™‘ï¼š**

- è¾“å…¥å­—æ®µå¯è‡ªå®šä¹‰

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬- è¾“å‡ºå­—æ®µä¿ç•™æ ‡å‡†æ ¼å¼

npm run build- å…¨å±€å˜é‡å¯åœ¨ç³»ç»Ÿé…ç½®ä¸­è®¾ç½®

```

---

### æŠ€æœ¯æ ˆ

## ğŸ“ æ›´æ–°æ—¥å¿—

**å‰ç«¯ï¼š**

- Vue 3 + Composition API- ç®€åŒ– LLM é…ç½®ï¼Œä»…æ”¯æŒ Ollama æœ¬åœ°æ¨¡å‹

- Vite 5.x- ç§»é™¤ç¬¬ä¸‰æ–¹ API ä¾èµ– (OpenAI, DeepSeek, Moonshot ç­‰)

- Naive UI- æ·»åŠ ä¸€é”®å¯åŠ¨è„šæœ¬

- Pinia- ä¼˜åŒ–åµŒå…¥æ¨¡å‹åŠ è½½

- Tauri 2.0 (å¯é€‰ï¼Œç”¨äºæ¡Œé¢åº”ç”¨)

---

**åç«¯ï¼š**

- Python 3.11+## ğŸ“„ License

- FastAPI

- SQLAlchemy + SQLiteMIT License

- LangChain

- ChromaDB---



---## ğŸ™ è‡´è°¢



## ğŸ“ å¸¸è§é—®é¢˜- [Ollama](https://ollama.ai/) - æœ¬åœ° LLM æœåŠ¡

- [M3E](https://huggingface.co/moka-ai/m3e-small) - ä¸­æ–‡åµŒå…¥æ¨¡å‹

<details>- [LangChain](https://langchain.com/) - LLM åº”ç”¨æ¡†æ¶

<summary><b>Q: å¯åŠ¨æ—¶æç¤ºç«¯å£è¢«å ç”¨ï¼Ÿ</b></summary>- [Naive UI](https://naiveui.com/) - Vue 3 ç»„ä»¶åº“

- [Tauri](https://tauri.app/) - æ¡Œé¢åº”ç”¨æ¡†æ¶

```bash
# æŸ¥çœ‹å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :6088
lsof -i :11420

# ç»ˆæ­¢è¿›ç¨‹
kill -9 <PID>
```
</details>

<details>
<summary><b>Q: Ollama æ¨¡å‹ä¸‹è½½æ…¢ï¼Ÿ</b></summary>

å¯ä»¥è®¾ç½®ä»£ç†æˆ–ä½¿ç”¨é•œåƒï¼š
```bash
export OLLAMA_HOST=https://your-mirror.com
ollama pull qwen2
```
</details>

<details>
<summary><b>Q: M3E æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ</b></summary>

æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæ¨¡å‹ä¼šä» HuggingFace è‡ªåŠ¨ä¸‹è½½ã€‚å¦‚æœç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½åˆ° `kai-server/resources/model/m3e/` ç›®å½•ã€‚
</details>

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

---

## ğŸ“„ License

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚

---

## ğŸ™ è‡´è°¢

- [Ollama](https://ollama.ai/) - æœ¬åœ° LLM æœåŠ¡
- [M3E](https://huggingface.co/moka-ai/m3e-small) - ä¸­æ–‡åµŒå…¥æ¨¡å‹
- [LangChain](https://langchain.com/) - LLM åº”ç”¨æ¡†æ¶
- [Naive UI](https://naiveui.com/) - Vue 3 ç»„ä»¶åº“
- [FastAPI](https://fastapi.tiangolo.com/) - Python Web æ¡†æ¶
