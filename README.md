# Overview# KAI - Personal Local RAG



KAI is a desktop application that enables users to build a personal knowledge base and perform intelligent Q&A over their documents. All data is stored locally, ensuring complete privacy without relying on cloud services.



# Features**Next-Generation Knowledge Management Tool Powered by LLM****Next-Generation Knowledge Management Tool Powered by LLM**



# Knowledge Base Management



- **Multi-format Document Support**: Import PDF, Word, Markdown, plain text, and web links[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)

- **Automatic Document Processing**: Intelligent text segmentation, metadata extraction, and indexing

- **Document Organization**: Hierarchical folder structure, tagging, and batch operations[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

- **Version Control**: Track document changes and maintain revision history

[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org) KAI - Personal Local RAG

# Intelligent Search and Q&A

[![Tauri](https://img.shields.io/badge/Tauri-2.0-orange.svg)](https://tauri.app)

- **Semantic Search**: Vector-based retrieval that understands context and meaning, not just keywords

- **Conversational Q&A**: Multi-turn dialogue with context awareness and follow-up question support[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

- **Source Citation**: All answers include references to original document sources

- **Cross-document Retrieval**: Search and synthesize information across multiple documents



### AI Integration[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)



- **Local LLM Support**: Integration with Ollama for fully offline operation

- **Multiple Model Providers**: Support for OpenAI, DeepSeek, Moonshot, Tongyi Qianwen, and Zhipu AI

- **Embedded Models**: Built-in M3E embedding model for semantic understanding</div>[![Tauri](https://img.shields.io/badge/Tauri-2.0-orange.svg)](https://tauri.app)

- **Flexible Configuration**: Easy switching between different models and providers



### Desktop Application

---[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)**Next-Generation Knowledge Management Tool Powered by LLM**# ğŸ“š KAI - Personal Local RAG

- **Cross-platform**: Built with Tauri 2.0 for Windows, macOS, and Linux

- **Native Performance**: Lightweight and fast with minimal resource usage

- **Offline Capable**: Full functionality without internet connection when using local models

## âœ¨ Features

## Technical Architecture



### Frontend

### ğŸ“– Smart Knowledge Base Management[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)

- Vue 3 with Composition API

- Vite 5.x for development and building- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more

- Naive UI component library

- Tauri 2.0 for desktop packaging- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction



### Backend- **Document Management** - Version control, knowledge linking, batch operations



- Python 3.11+ with FastAPI- **Dual-mode Editor** - Markdown + Rich text hybrid editing---[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)

- SQLAlchemy for database management

- LangChain for LLM orchestration

- ChromaDB for vector storage

### ğŸ” Intelligent Search & Q&A

## Installation

- **Semantic Search** - Deep cross-document retrieval based on vector database

### Prerequisites

- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning## âœ¨ Features[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

- Python 3.11 or higher

- Node.js 18.x or higher- **Local Deployment** - Fully local data storage, privacy protection

- Ollama (optional, for local LLM)



### Setup

# AI Capabilities

1. Clone the repository:

- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, DeepSeek, Mistral and more### ğŸ“– Smart Knowledge Base Management[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)**Next-Generation Knowledge Management Tool Powered by LLM**# ğŸ“š KAI - Intelligent Personal Knowledge Base is a next-generation knowledge management tool based on large language models (LLMS), integrating AI capabilities to redefine the way knowledge is organized. Supports multi-source knowledge integration, intelligent question answering, automated knowledge processing and visual knowledge networks, helping to build your second brain.

```bash

git clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git- **Embedding Model** - Built-in M3E embedding model for semantic understanding

cd Kai-personalLocalRAG

```- **Multiple Providers** - Support for OpenAI, DeepSeek, Moonshot, Tongyi, Zhipu AI- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more



2. Start the backend server:



```bash---- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

cd kai-server

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt## Quick Start- **Document Management** - Version control, knowledge linking, batch operations

python app.py

```



3. Start the frontend:### Requirements- **Dual-mode Editor** - Markdown + Rich text hybrid editing



```bash

cd kai-client

npm install| Dependency | Version | Description |

npm run dev

```|------------|---------|-------------|



4. Access the application at `http://localhost:11420`| Python | >= 3.11 | Backend runtime |### ğŸ” Intelligent Search & Q&A[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)



### Quick Start| Node.js | >= 18.x | Frontend runtime |



Use the provided scripts for convenience:| Ollama | Latest | Local LLM service (optional) |- **Semantic Search** - Deep cross-document retrieval based on vector database



```bash

./start.sh  # Start all services

./stop.sh   # Stop all services### 1. Clone the Repository- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)

```



## Configuration

```bash- **Local Deployment** - Fully local data storage, privacy protection

### LLM Provider Setup

git clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git

1. Navigate to Settings > LLM

2. Select a provider (Ollama, OpenAI, DeepSeek, etc.)cd Kai-personalLocalRAG</div>

3. Enter the API key or base URL

4. Click "Sync Models" to fetch available models```



### Ollama Configuration### AI Capabilities



Default endpoint: `http://127.0.0.1:11434`### 2. Install Ollama (Optional)



Recommended models:- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, DeepSeek, Mistral and more[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)

- qwen2.5:7b - General purpose

- llama3.2:3b - Lightweight optionDownload from [ollama.ai](https://ollama.ai) and install, then pull a model:

- deepseek-r1:8b - Reasoning tasks

- **Embedding Model** - Built-in M3E embedding model for better semantic understanding

## Innovations

```bash

### Privacy-First Design

ollama pull qwen2.5:7b- **Multiple Providers** - Support for OpenAI, DeepSeek, Moonshot, Tongyi, Zhipu AI---

Unlike cloud-based solutions, KAI stores all data locally. Documents, vectors, and conversation history never leave the user's machine, making it suitable for sensitive or confidential information.

# or

### Hybrid Retrieval

ollama pull llama3.2:3b

Combines keyword-based and semantic vector search for improved accuracy. The system automatically balances between exact matching and contextual understanding based on query characteristics.

```

### Adaptive Document Processing

---[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)**Next-generation personal local knowledge management tools based on large language models**#

The system analyzes document structure and content type to apply appropriate segmentation strategies. Technical documents, narratives, and structured data are processed differently to preserve context and meaning.

### 3. Start the Application

### Local-First with Cloud Option



Designed to work completely offline with Ollama, while also supporting cloud providers for users who prefer hosted solutions. Users can switch between local and cloud models without data migration.

**Option A: Quick Start Script**

### Lightweight Desktop Packaging

## Quick Start## Features

Built with Tauri instead of Electron, resulting in significantly smaller application size and lower memory footprint while maintaining full cross-platform compatibility.

```bash

## Project Structure

chmod +x start.sh

```

Kai-personalLocalRAG/./start.sh

â”œâ”€â”€ kai-client/           # Frontend application

â”‚   â”œâ”€â”€ src/```### Requirements[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

â”‚   â”‚   â”œâ”€â”€ views/        # Page components

â”‚   â”‚   â”œâ”€â”€ components/   # Reusable UI components

â”‚   â”‚   â”œâ”€â”€ store/        # State management

â”‚   â”‚   â””â”€â”€ router/       # Routing configuration**Option B: Manual Start**

â”‚   â””â”€â”€ src-tauri/        # Tauri configuration

â”œâ”€â”€ kai-server/           # Backend server

â”‚   â”œâ”€â”€ server/

â”‚   â”‚   â”œâ”€â”€ api/          # REST API endpointsBackend:| Dependency | Version | Description |### ğŸ“– Smart Knowledge Base Management

â”‚   â”‚   â”œâ”€â”€ core/         # Business logic

â”‚   â”‚   â”œâ”€â”€ db/           # Database operations```bash

â”‚   â”‚   â””â”€â”€ model/        # Data models

â”‚   â”œâ”€â”€ config/           # Configuration filescd kai-server|------------|---------|-------------|

â”‚   â””â”€â”€ resources/        # Static resources and data

â”œâ”€â”€ start.sh              # Startup scriptpython -m venv venv

â””â”€â”€ stop.sh               # Shutdown script

```source venv/bin/activate  # Windows: venv\Scripts\activate| Python | >= 3.11 | Backend runtime |- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more



## Licensepip install -r requirements.txt



MIT Licensepython app.py| Node.js | >= 18.x | Frontend runtime |



## Author```



kaiyu-li317 - https://github.com/kaiyu-li317| Ollama | Latest | Local LLM service (optional) |- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction


Frontend:

```bash

cd kai-client

npm install### 1. Clone the Repository- **Document Management** - Version control, knowledge linking, batch operations[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Project Structure](#-project-structure) â€¢ [Configuration](#-configuration) â€¢ [Development](#-development)

npm run dev

```



### 4. Access the Application```bash- **Dual-mode Editor** - Markdown + Rich text hybrid editing



Open browser: `http://localhost:11420`git clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git



---cd Kai-personalLocalRAG[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)### æ™ºèƒ½çŸ¥è¯†åº“ç®¡ç†



## ğŸ“ Project Structure```



```### ğŸ” Intelligent Search & Q&A

Kai-personalLocalRAG/

â”œâ”€â”€ kai-client/              # Frontend (Vue 3 + Vite + Tauri)### 2. Install Ollama (Optional, for local LLM)

â”‚   â”œâ”€â”€ src/

â”‚   â”‚   â”œâ”€â”€ views/           # Page components- **Semantic Search** - Deep cross-document retrieval based on vector database</div>

â”‚   â”‚   â”œâ”€â”€ components/      # Reusable components

â”‚   â”‚   â”œâ”€â”€ store/           # State managementDownload from [ollama.ai](https://ollama.ai) and install, then pull a model:

â”‚   â”‚   â””â”€â”€ router/          # Routing

â”‚   â””â”€â”€ src-tauri/           # Tauri desktop config- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning

â”‚

â”œâ”€â”€ kai-server/              # Backend (Python + FastAPI)```bash

â”‚   â”œâ”€â”€ server/

â”‚   â”‚   â”œâ”€â”€ api/             # API endpointsollama pull qwen2.5:7b- **Local Deployment** - Fully local data storage, privacy protection[![Vue](https://img.shields.io/badge/Vue-3.x-green.svg)](https://vuejs.org)- **å¤šæ ¼å¼å¯¼å…¥**ï¼šæ”¯æŒæ–‡æ¡£ï¼ˆPDF/Word/Markdownï¼‰ã€ç½‘é¡µé“¾æ¥ã€çº¯æ–‡æœ¬ç­‰å¤šæºæ•°æ®æ¥å…¥

â”‚   â”‚   â”œâ”€â”€ core/            # Business logic

â”‚   â”‚   â”œâ”€â”€ db/              # Database management# or

â”‚   â”‚   â””â”€â”€ model/           # Data models

â”‚   â”œâ”€â”€ config/              # Configurationollama pull llama3.2:3b

â”‚   â””â”€â”€ resources/           # Static resources

â”‚```

â”œâ”€â”€ start.sh                 # Quick start script

â””â”€â”€ stop.sh                  # Stop services script### ğŸ¤– AI Capabilities---

```

### 3. Start the Application

---

- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, Mistral and more

## Configuration

#### Option A: Quick Start Script

### LLM Provider Setup

- **Chinese Optimized** - Built-in M3E Chinese embedding model for better semantic understanding[![FastAPI](https://img.shields.io/badge/FastAPI-0.112-009688.svg)](https://fastapi.tiangolo.com)- **AI è‡ªåŠ¨åŒ–å¤„ç†**ï¼šè‡ªåŠ¨åˆ†æ®µã€ç”Ÿæˆæ‘˜è¦ã€åˆ›å»º Q&A å¯¹ã€æå–çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„

1. Go to **Settings** > **LLM**

2. Select a provider (Ollama, OpenAI, DeepSeek, etc.)```bash

3. Configure API key or base URL

4. Click **Sync Models** to load available modelschmod +x start.sh



### Ollama Configuration./start.sh



Default base URL: `http://127.0.0.1:11434````---## âœ¨ Features



Recommended models:

- `qwen2.5:7b` - General use

- `llama3.2:3b` - Lightweight#### Option B: Manual Start

- `deepseek-r1:8b` - Reasoning tasks



---

**Backend:**## Quick Start[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)- **åŠ¨æ€ç»´æŠ¤**ï¼šæ”¯æŒæ–‡æ¡£ç‰ˆæœ¬ç®¡ç†ã€çŸ¥è¯†å…³è”æ ‡æ³¨ã€æ‰¹é‡å¤„ç†æ“ä½œ

## Development

```bash

### Tech Stack

cd kai-server

**Frontend:**

- Vue 3 + Composition APIpython -m venv venv

- Vite 5.x

- Naive UIsource venv/bin/activate  # On Windows: venv\Scripts\activate### Requirements### ğŸ“– Smart Knowledge Base Management

- Tauri 2.0 (Desktop)

pip install -r requirements.txt

**Backend:**

- Python 3.11+python app.py

- FastAPI

- SQLAlchemy```

- LangChain

- ChromaDB (Vector Store)| Dependency | Version | Description |- **Multi-format Import** - Support PDF, Word, Markdown, web links, plain text and more- **Dual-mode editor**ï¼šMarkdown + Rich text mixed editing



### Build Desktop App**Frontend:**



```bash```bash|------------|---------|-------------|

cd kai-client

npm run tauri buildcd kai-client

```

npm install| Python | >= 3.11 | Backend runtime |- **AI Auto-processing** - Automatic segmentation, summary generation, Q&A pair creation, knowledge extraction

---

npm run dev

## ğŸ“„ License

```| Node.js | >= 18.x | Frontend runtime |

MIT License - see [LICENSE](LICENSE) for details.



---

### 4. Access the Application| Ollama | Latest | Local LLM service |- **Document Management** - Version control, knowledge linking, Batch operations [features] (# - features), [quick start] (# - quick start), [projects] structure (# - project structure), [configuration instructions] (# - configuration instructions), [] development guide (# - development guide)

## Contributing



1. Fork the repository

2. Create feature branch (`git checkout -b feature/AmazingFeature`)Open your browser and navigate to: `http://localhost:11420`

3. Commit changes (`git commit -m 'Add AmazingFeature'`)

4. Push to branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request

---### 1. Clone the Repository- **Dual-mode Editor** - Markdown + Rich text hybrid editing

---



## Contact
email: kaiyuli317@gmail.com

## Project Structure

- GitHub: [@kaiyu-li317](https://github.com/kaiyu-li317)

- Repository: [Kai-personalLocalRAG](https://github.com/kaiyu-li317/Kai-personalLocalRAG)

kai-main/

â”œâ”€â”€ kai-client/          # Frontend (Vue 3 + Vite + Tauri)git clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git

â”‚   â”œâ”€â”€ src/

â”‚   â”‚   â”œâ”€â”€ views/       # Page componentscd Kai-personalLocalRAG### ğŸ” Intelligent Search & Q&A

â”‚   â”‚   â”œâ”€â”€ components/  # Reusable components

â”‚   â”‚   â”œâ”€â”€ store/       # State management```

â”‚   â”‚   â””â”€â”€ router/      # Routing

â”‚   â””â”€â”€ src-tauri/       # Tauri desktop app config- **Semantic Search** - Deep cross-document retrieval based on vector database</div>- ** Semantic Search ** : Cross-document deep retrieval based on vector databases

â”‚

â”œâ”€â”€ kai-server/          # Backend (Python + FastAPI)### 2. Install Ollama

â”‚   â”œâ”€â”€ server/

â”‚   â”‚   â”œâ”€â”€ api/         # API endpoints- **Conversational Q&A** - Follow-up questions, source citation, multi-turn reasoning

â”‚   â”‚   â”œâ”€â”€ core/        # Core business logic

â”‚   â”‚   â”œâ”€â”€ db/          # Database management```bash

â”‚   â”‚   â””â”€â”€ model/       # Data models

â”‚   â”œâ”€â”€ config/          # Configuration files# macOS- **Local Deployment** - Fully local data storage, privacy protection- ** Conversational interaction ** : Supports follow-up questioning, source tracing and citation, and multi-round knowledge reasoning

â”‚   â””â”€â”€ resources/       # Static resources & database

â”‚brew install ollama

â”œâ”€â”€ start.sh             # Quick start script

â””â”€â”€ stop.sh              # Stop services script

```

# Linux

---

curl -fsSL https://ollama.com/install.sh | sh# AI Capabilities---

# Configuration



### LLM Provider Setup

# Start service and download model- **Local LLM** - Integrated with Ollama, supports Qwen, Llama, Mistral and more

1. Go to **Settings** > **LLM**

2. Select a provider (Ollama, OpenAI, DeepSeek, etc.)ollama serve

3. Configure API key or base URL

4. Click **Sync Models** to load available modelsollama pull qwen2- **Chinese Optimized** - Built-in M3E Chinese embedding model for better semantic understanding---



### Ollama Configuration```



Default base URL: `http://127.0.0.1:11434`



Supported models:### 3. One-Click Start

- `qwen2.5:7b` - Recommended for general use

- `llama3.2:3b` - Lightweight option---

- `deepseek-r1:8b` - Good for reasoning tasks

```bash

---

# Add execute permission

## ğŸ› ï¸ Development

chmod +x start.sh stop.sh

### Tech Stack

## Quick Start## 

**Frontend:**

- Vue 3 + Composition API# Start all services

- Vite 5.x

- Naive UI./start.sh

- Tauri 2.0 (Desktop)

```

**Backend:**

- Python 3.11+### Requirements### intelligent knowledge base management

- FastAPI

- SQLAlchemy### 4. Access the Application

- LangChain

- ChromaDB (Vector Store)



### Build Desktop App- **Frontend**: http://localhost:11420



```bash- **Backend API**: http://localhost:6088| Dependency | Version | Description |- ** Multi-format import ** - Supports PDF, Word, Markdown, web links, plain text and other multi-source data

cd kai-client

npm run tauri build- **API Docs**: http://localhost:6088/docs

```

|------------|---------|-------------|

---

### 5. Stop Services

## License

| Python | >= 3.11 | Backend runtime |- **AI è‡ªåŠ¨å¤„ç†** - è‡ªåŠ¨åˆ†æ®µã€ç”Ÿæˆæ‘˜è¦ã€åˆ›å»º Q&A å¯¹ã€æå–çŸ¥è¯†è¦ç‚¹kai-main/

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```bash

---

./stop.sh| Node.js | >= 18.x | Frontend runtime |


## ğŸ“ Project Structure

---

### 1. Clone the Repository- **åŒæ¨¡å¼ç¼–è¾‘** - Markdown + å¯Œæ–‡æœ¬æ··åˆç¼–è¾‘å™¨â”‚   â”œâ”€â”€ src/

## Contact

```

- GitHub: [@kaiyu-li317](https://github.com/kaiyu-li317)

- Repository: [Kai-personalLocalRAG](https://github.com/kaiyu-li317/Kai-personalLocalRAG)kai-main/


â”œâ”€â”€ kai-client/              # Frontend (Vue 3 + Vite + Tauri)

â”‚   â”œâ”€â”€ src/```bashâ”‚   â”‚   â”œâ”€â”€ views/         # é¡µé¢ç»„ä»¶

â”‚   â”‚   â”œâ”€â”€ views/           # Page components

â”‚   â”‚   â”œâ”€â”€ components/      # Reusable componentsgit clone https://github.com/kaiyu-li317/Kai-personalLocalRAG.git

â”‚   â”‚   â”œâ”€â”€ store/           # State management

â”‚   â”‚   â”œâ”€â”€ libs/            # Utility librariescd Kai-personalLocalRAG### ğŸ” æ™ºèƒ½æœç´¢ä¸é—®ç­”â”‚   â”‚   â”œâ”€â”€ components/    # é€šç”¨ç»„ä»¶

â”‚   â”‚   â””â”€â”€ router/          # Route configuration

â”‚   â””â”€â”€ src-tauri/           # Tauri desktop app config```

â”‚

â”œâ”€â”€ kai-server/              # Backend (Python FastAPI)- **è¯­ä¹‰æœç´¢** - åŸºäºå‘é‡æ•°æ®åº“çš„è·¨æ–‡æ¡£æ·±åº¦æ£€ç´¢â”‚   â”‚   â”œâ”€â”€ store/         # çŠ¶æ€ç®¡ç†

â”‚   â”œâ”€â”€ server/

â”‚   â”‚   â”œâ”€â”€ api/             # API endpoints### 2. Install Ollama

â”‚   â”‚   â”œâ”€â”€ core/            # Core business logic

â”‚   â”‚   â”œâ”€â”€ db/              # Database operations- **å¯¹è¯å¼é—®ç­”** - æ”¯æŒè¿½é—®ã€æº¯æºå¼•ç”¨ã€å¤šè½®çŸ¥è¯†æ¨ç†â”‚   â”‚   â””â”€â”€ router/        # è·¯ç”±é…ç½®

â”‚   â”‚   â””â”€â”€ model/           # Data models

â”‚   â”œâ”€â”€ config/              # Configuration files```bash

â”‚   â””â”€â”€ resources/           # Resource files

â”‚# macOS- **æœ¬åœ°éƒ¨ç½²** - æ•°æ®å®Œå…¨æœ¬åœ°åŒ–ï¼Œä¿æŠ¤éšç§å®‰å…¨â”‚   â””â”€â”€ src-tauri/         # Tauri æ¡Œé¢åº”ç”¨é…ç½®

â”œâ”€â”€ start.sh                 # Startup script

â”œâ”€â”€ stop.sh                  # Shutdown scriptbrew install ollama

â””â”€â”€ README.md

```â”‚



---# Linux



## âš™ï¸ Configurationcurl -fsSL https://ollama.com/install.sh | sh### ğŸ¤– AI èƒ½åŠ›â”œâ”€â”€ wenkb-server/          # åç«¯ (Python FastAPI)



### LLM Configuration



Edit `kai-server/config/llm.py`:# Start service and download model- **æœ¬åœ° LLM** - é›†æˆ Ollamaï¼Œæ”¯æŒ Qwenã€Llamaã€Mistral ç­‰æ¨¡å‹â”‚   â”œâ”€â”€ server/



```pythonollama serve

# Ollama service address

OLLAMA_HOST = "http://localhost:11434"ollama pull qwen2- **ä¸­æ–‡ä¼˜åŒ–** - å†…ç½® M3E ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œè¯­ä¹‰ç†è§£æ›´ç²¾å‡†â”‚   â”‚   â”œâ”€â”€ api/           # API æ¥å£



# Default LLM model```

DEFAULT_LLM_MODEL = "qwen2"

â”‚   â”‚   â”œâ”€â”€ core/          # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

# Embedding model path

EMBEDDING_MODEL_PATH = "./resources/model/m3e"### 3. One-Click Start

```

---â”‚   â”‚   â”œâ”€â”€ db/            # æ•°æ®åº“æ“ä½œ

### Database Configuration

```bash

Edit `kai-server/config/datasource.py`:

# Add execute permissionâ”‚   â”‚   â””â”€â”€ model/         # æ•°æ®æ¨¡å‹

```python

# SQLite database pathchmod +x start.sh stop.sh

DATABASE_PATH = "./resources/database/kai.db"

## ğŸš€ å¿«é€Ÿå¼€å§‹â”‚   â”œâ”€â”€ config/            # é…ç½®æ–‡ä»¶

# Vector store path

VECTOR_STORE_PATH = "./resources/vector_store"# Start all services

```

./start.shâ”‚   â””â”€â”€ resources/         # èµ„æºæ–‡ä»¶

### Frontend Configuration



Edit `kai-client/vite.config.js`:

# Check service status### ç¯å¢ƒè¦æ±‚â”‚       â”œâ”€â”€ database/      # SQLite æ•°æ®åº“

```javascript

// API proxy configuration./start.sh status

proxy: {

  '/api': {â”‚       â””â”€â”€ model/         # æœ¬åœ°åµŒå…¥æ¨¡å‹ (m3e)

    target: 'http://localhost:6088',

    changeOrigin: true# Stop services

  }

}./stop.sh| ä¾èµ– | ç‰ˆæœ¬ | è¯´æ˜ |â”‚

```

```

---

|------|------|------|â”œâ”€â”€ start.sh               # ä¸€é”®å¯åŠ¨è„šæœ¬

## ğŸ› ï¸ Development

### 4. Access the Application

### Backend Development

| Python | >= 3.11 | åç«¯è¿è¡Œç¯å¢ƒ |â””â”€â”€ stop.sh                # åœæ­¢è„šæœ¬

```bash

cd kai-serverAfter successful startup, visit: **http://localhost:11420**



# Create virtual environment| Node.js | >= 18.x | å‰ç«¯è¿è¡Œç¯å¢ƒ |```

python -m venv venv

source venv/bin/activate  # Windows: venv\Scripts\activate| Service | Port | Description |



# Install dependencies|---------|------|-------------|| Ollama | æœ€æ–°ç‰ˆ | æœ¬åœ° LLM æœåŠ¡ |

pip install -r requirements.txt

| Frontend | 11420 | Web interface |

# Start development server

python app.py| Backend | 6088 | API service |---

```

| Ollama | 11434 | LLM service |

### Frontend Development

### 1. å…‹éš†é¡¹ç›®

```bash

cd kai-client---



# Install dependencies## ğŸš€ å¿«é€Ÿå¼€å§‹

npm install

## ğŸ“ Project Structure

# Start development server

npm run dev```bash



# Build for production```

npm run build

```kai/git clone https://github.com/yourusername/kai.git### ç¯å¢ƒè¦æ±‚



### Desktop App (Tauri)â”œâ”€â”€ kai-client/                # Frontend project



```bashâ”‚   â”œâ”€â”€ src/cd kai

cd kai-client

â”‚   â”‚   â”œâ”€â”€ views/            # Page components

# Install Tauri CLI

npm install -g @tauri-apps/cliâ”‚   â”‚   â”œâ”€â”€ components/       # Common components```- **Node.js** >= 18.x



# Development modeâ”‚   â”‚   â”œâ”€â”€ store/            # Pinia state management

npm run tauri dev

â”‚   â”‚   â”œâ”€â”€ router/           # Vue Router- **Python** >= 3.11

# Build desktop app

npm run tauri buildâ”‚   â”‚   â””â”€â”€ config/           # Frontend config

```

â”‚   â”œâ”€â”€ src-tauri/            # Tauri desktop app config### 2. å®‰è£… Ollama- **Ollama** (æœ¬åœ° LLM æœåŠ¡)

---

â”‚   â”œâ”€â”€ package.json

## ğŸ“‹ Tech Stack

â”‚   â””â”€â”€ vite.config.js

### Backend

- **Framework**: FastAPIâ”‚

- **Database**: SQLite + SQLAlchemy

- **Vector Store**: ChromaDBâ”œâ”€â”€ kai-server/                # Backend project```bash### ä¸€é”®å¯åŠ¨

- **LLM**: Ollama (Qwen2, Llama, etc.)

- **Embedding**: M3E (Local model)â”‚   â”œâ”€â”€ server/

- **Document Processing**: LangChain

â”‚   â”‚   â”œâ”€â”€ api/              # API layer# macOS

### Frontend

- **Framework**: Vue 3 + Composition APIâ”‚   â”‚   â”œâ”€â”€ core/             # Core business logic

- **Build Tool**: Vite 5.x

- **UI Library**: Naive UIâ”‚   â”‚   â”œâ”€â”€ db/               # Database operationsbrew install ollama```bash

- **Desktop**: Tauri 2.0

- **State Management**: Piniaâ”‚   â”‚   â”œâ”€â”€ model/            # Data models



---â”‚   â”‚   â””â”€â”€ utils/            # Utility functionscd kai-main



## â“ FAQâ”‚   â”œâ”€â”€ config/               # Configuration files



### Q: How to change the LLM model?â”‚   â”œâ”€â”€ resources/            # Resource directory# Linux

A: Modify `DEFAULT_LLM_MODEL` in `kai-server/config/llm.py`, or configure through the web interface in Settings.

â”‚   â”‚   â”œâ”€â”€ database/         # SQLite database

### Q: Why is semantic search slow?

A: First search requires loading the embedding model. Subsequent searches will be faster.â”‚   â”‚   â”œâ”€â”€ documents/        # Uploaded documentscurl -fsSL https://ollama.com/install.sh | sh# å¯åŠ¨æ‰€æœ‰æœåŠ¡ (Ollama + åç«¯ + å‰ç«¯)



### Q: How to import large documents?â”‚   â”‚   â”œâ”€â”€ model/            # Embedding model

A: The system automatically chunks large documents. You can adjust chunk size in configuration.

â”‚   â”‚   â””â”€â”€ vector_store/     # Vector storage./start.sh

### Q: Does it support GPU acceleration?

A: Yes, if Ollama is configured with GPU support, LLM inference will be accelerated automatically.â”‚   â”œâ”€â”€ app.py                # Application entry



---â”‚   â””â”€â”€ requirements.txt# å¯åŠ¨æœåŠ¡å¹¶ä¸‹è½½æ¨¡å‹



## ğŸ¤ Contributingâ”‚



Contributions are welcome! Please feel free to submit a Pull Request.â”œâ”€â”€ start.sh                   # One-click start scriptollama serve# åœæ­¢æ‰€æœ‰æœåŠ¡



1. Fork the repositoryâ”œâ”€â”€ stop.sh                    # Stop script

2. Create your feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)â””â”€â”€ README.mdollama pull qwen2./stop.sh

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Open a Pull Request```



---```



## ğŸ“„ License---



This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.# é‡å¯æœåŠ¡



---## âš™ï¸ Configuration



<div align="center">### 3. ä¸€é”®å¯åŠ¨./start.sh restart



**Built with â¤ï¸ by [kaiyu-li317](https://github.com/kaiyu-li317)**### Backend Configuration



</div>


Configuration files are located in `kai-server/config/`:

```bash# æŸ¥çœ‹æœåŠ¡çŠ¶æ€

| File | Description |

|------|-------------|# æ·»åŠ æ‰§è¡Œæƒé™./start.sh status

| `llm.py` | LLM model config (Ollama address, model name) |

| `common.py` | Common config (port, paths, etc.) |chmod +x start.sh stop.sh```

| `datasource.py` | Data source config (database connection) |



### Embedding Model

# å¯åŠ¨æ‰€æœ‰æœåŠ¡### æœåŠ¡ç«¯å£

The project uses **M3E** Chinese embedding model, which will be automatically downloaded to `kai-server/resources/model/m3e/` on first startup.

./start.sh

| Model | Parameters | Vector Dim | Features |

|-------|------------|------------|----------|| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |

| m3e-small | 24M | 512 | Lightweight, Chinese optimized |

| m3e-base | 110M | 768 | Higher accuracy, bilingual |# æŸ¥çœ‹æœåŠ¡çŠ¶æ€|------|------|------|



### LLM Models./start.sh status| å‰ç«¯ | 11420 | Vue 3 å¼€å‘æœåŠ¡å™¨ |



Recommended Ollama models:| åç«¯ | 6088 | FastAPI æœåŠ¡ |



```bash# åœæ­¢æœåŠ¡| Ollama | 11434 | æœ¬åœ° LLM æœåŠ¡ |

ollama pull qwen2        # Recommended for Chinese

ollama pull llama3.1     # Best for English./stop.sh

ollama pull mistral      # Balanced choice

``````è®¿é—®åœ°å€ï¼šhttp://localhost:11420



---



## ğŸ› ï¸ Development### 4. è®¿é—®åº”ç”¨---



### Manual Start



**Backend:**å¯åŠ¨æˆåŠŸåè®¿é—®ï¼š**http://localhost:11420**## ğŸ”§ æ‰‹åŠ¨å®‰è£…



```bash

cd kai-server

| æœåŠ¡ | ç«¯å£ | è¯´æ˜ |### 1. å®‰è£… Ollama

# Create virtual environment

python -m venv .venv|------|------|------|

source .venv/bin/activate  # Windows: .venv\Scripts\activate

| å‰ç«¯ | 11420 | Web ç•Œé¢ |```bash

# Install dependencies

pip install -r requirements.txt| åç«¯ | 6088 | API æœåŠ¡ |# macOS



# Start service| Ollama | 11434 | LLM æœåŠ¡ |brew install ollama

python app.py

```



**Frontend:**---# å¯åŠ¨æœåŠ¡å¹¶ä¸‹è½½æ¨¡å‹



```bashollama serve

cd kai-client

## ğŸ“ é¡¹ç›®ç»“æ„ollama pull qwen2

# Install dependencies

npm install```



# Start dev server```

npm run dev

kai/### 2. åç«¯æœåŠ¡

# Build for production

npm run buildâ”œâ”€â”€ kai-client/                # å‰ç«¯é¡¹ç›®

```

â”‚   â”œâ”€â”€ src/```bash

### Tech Stack

â”‚   â”‚   â”œâ”€â”€ views/            # é¡µé¢ç»„ä»¶cd wenkb-server

**Frontend:**

- Vue 3 + Composition APIâ”‚   â”‚   â”œâ”€â”€ components/       # é€šç”¨ç»„ä»¶

- Vite 5.x

- Naive UIâ”‚   â”‚   â”œâ”€â”€ store/            # Pinia çŠ¶æ€ç®¡ç†# å®‰è£…ä¾èµ–

- Pinia

- Tauri 2.0 (optional, for desktop app)â”‚   â”‚   â”œâ”€â”€ router/           # Vue Router è·¯ç”±pip install -r requirements.txt



**Backend:**â”‚   â”‚   â””â”€â”€ config/           # å‰ç«¯é…ç½®

- Python 3.11+

- FastAPIâ”‚   â”œâ”€â”€ src-tauri/            # Tauri æ¡Œé¢åº”ç”¨é…ç½®# å¯åŠ¨æœåŠ¡

- SQLAlchemy + SQLite

- LangChainâ”‚   â”œâ”€â”€ package.jsonpython app.py

- ChromaDB

â”‚   â””â”€â”€ vite.config.js# æˆ–

---

â”‚uvicorn app:app --host 0.0.0.0 --port 6088 --reload

## ğŸ“ FAQ

â”œâ”€â”€ kai-server/                # åç«¯é¡¹ç›®```

<details>

<summary><b>Q: Port already in use?</b></summary>â”‚   â”œâ”€â”€ server/



```bashâ”‚   â”‚   â”œâ”€â”€ api/              # API æ¥å£å±‚### 3. å‰ç«¯æœåŠ¡

# Find process using the port

lsof -i :6088â”‚   â”‚   â”œâ”€â”€ core/             # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

lsof -i :11420

â”‚   â”‚   â”œâ”€â”€ db/               # æ•°æ®åº“æ“ä½œ```bash

# Kill the process

kill -9 <PID>â”‚   â”‚   â”œâ”€â”€ model/            # æ•°æ®æ¨¡å‹cd wenkb-client

```

</details>â”‚   â”‚   â””â”€â”€ utils/            # å·¥å…·å‡½æ•°



<details>â”‚   â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶# å®‰è£…ä¾èµ–

<summary><b>Q: Ollama model download is slow?</b></summary>

â”‚   â”œâ”€â”€ resources/            # èµ„æºç›®å½•npm install

You can set up a proxy or use a mirror:

```bashâ”‚   â”‚   â”œâ”€â”€ database/         # SQLite æ•°æ®åº“

export OLLAMA_HOST=https://your-mirror.com

ollama pull qwen2â”‚   â”‚   â”œâ”€â”€ documents/        # ä¸Šä¼ æ–‡æ¡£# å¯åŠ¨å¼€å‘æœåŠ¡å™¨

```

</details>â”‚   â”‚   â”œâ”€â”€ model/            # åµŒå…¥æ¨¡å‹npm run dev



<details>â”‚   â”‚   â””â”€â”€ vector_store/     # å‘é‡å­˜å‚¨```

<summary><b>Q: M3E model loading failed?</b></summary>

â”‚   â”œâ”€â”€ app.py                # åº”ç”¨å…¥å£

Check your network connection. The model will be automatically downloaded from HuggingFace. If there are network issues, you can manually download it to the `kai-server/resources/model/m3e/` directory.

</details>â”‚   â””â”€â”€ requirements.txt---



---â”‚



## ğŸ¤ Contributingâ”œâ”€â”€ start.sh                   # ä¸€é”®å¯åŠ¨è„šæœ¬## ğŸ¤– AI æ¨¡å‹é…ç½®



Contributions are welcome! Feel free to submit Issues and Pull Requests.â”œâ”€â”€ stop.sh                    # åœæ­¢è„šæœ¬



1. Fork this repositoryâ””â”€â”€ README.md### LLM æ¨¡å‹ (Ollama)

2. Create a feature branch (`git checkout -b feature/AmazingFeature`)

3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)```

4. Push to the branch (`git push origin feature/AmazingFeature`)

5. Create a Pull Requestæœ¬é¡¹ç›®ä½¿ç”¨ Ollama ä½œä¸ºæœ¬åœ° LLM æœåŠ¡ï¼Œé»˜è®¤ä½¿ç”¨ `qwen2` æ¨¡å‹ã€‚



------



## ğŸ“„ License```bash



This project is licensed under the [MIT License](LICENSE).## âš™ï¸ é…ç½®è¯´æ˜# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹



---ollama list



## ğŸ™ Acknowledgements### åç«¯é…ç½®



- [Ollama](https://ollama.ai/) - Local LLM service# å®‰è£…å…¶ä»–æ¨¡å‹

- [M3E](https://huggingface.co/moka-ai/m3e-small) - Chinese embedding model

- [LangChain](https://langchain.com/) - LLM application frameworké…ç½®æ–‡ä»¶ä½äº `kai-server/config/` ç›®å½•ï¼šollama pull llama3.1

- [Naive UI](https://naiveui.com/) - Vue 3 component library

- [FastAPI](https://fastapi.tiangolo.com/) - Python web frameworkollama pull mistral


| æ–‡ä»¶ | è¯´æ˜ |```

|------|------|

| `llm.py` | LLM æ¨¡å‹é…ç½® (Ollama åœ°å€ã€æ¨¡å‹åç§°) |### åµŒå…¥æ¨¡å‹ (M3E)

| `common.py` | é€šç”¨é…ç½® (ç«¯å£ã€è·¯å¾„ç­‰) |

| `datasource.py` | æ•°æ®æºé…ç½® (æ•°æ®åº“è¿æ¥) |é¡¹ç›®å†…ç½® `m3e-small` ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œç”¨äºæ–‡æ¡£å‘é‡åŒ–å’Œè¯­ä¹‰æœç´¢ã€‚



### åµŒå…¥æ¨¡å‹**M3E æ¨¡å‹ç‰¹ç‚¹ï¼š**

- æ”¯æŒä¸­è‹±åŒè¯­

é¡¹ç›®ä½¿ç”¨ **M3E** ä¸­æ–‡åµŒå…¥æ¨¡å‹ï¼Œé¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° `kai-server/resources/model/m3e/`ã€‚- åƒä¸‡çº§ä¸­æ–‡å¥å¯¹æ•°æ®é›†è®­ç»ƒ

- 512 ç»´å‘é‡è¾“å‡º

| æ¨¡å‹ | å‚æ•°é‡ | å‘é‡ç»´åº¦ | ç‰¹ç‚¹ |- é€‚ç”¨äºæ–‡æœ¬ç›¸ä¼¼åº¦ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡

|------|--------|----------|------|

| m3e-small | 24M | 512 | è½»é‡å¿«é€Ÿï¼Œä¸­æ–‡ä¼˜åŒ– || æ¨¡å‹ | å‚æ•°é‡ | ç»´åº¦ | ä¸­æ–‡ | è‹±æ–‡ |

| m3e-base | 110M | 768 | ç²¾åº¦æ›´é«˜ï¼Œä¸­è‹±åŒè¯­ ||------|--------|------|------|------|

| m3e-small | 24M | 512 | âœ… | âŒ |

### LLM æ¨¡å‹| m3e-base | 110M | 768 | âœ… | âœ… |



æ¨èä½¿ç”¨çš„ Ollama æ¨¡å‹ï¼š---



```bash## ğŸ“ é…ç½®æ–‡ä»¶è¯´æ˜

ollama pull qwen2        # æ¨èï¼Œä¸­æ–‡æ•ˆæœæœ€ä½³

ollama pull llama3.1     # è‹±æ–‡æ•ˆæœå¥½### åç«¯é…ç½®

ollama pull mistral      # å¹³è¡¡é€‰æ‹©

```- `config/llm.py` - LLM æ¨¡å‹é…ç½® (Ollama)

- `config/common.py` - é€šç”¨é…ç½®

---- `config/datasource.py` - æ•°æ®æºé…ç½®



## ğŸ› ï¸ å¼€å‘æŒ‡å—### å‰ç«¯é…ç½®



### æ‰‹åŠ¨å¯åŠ¨- `vite.config.js` - Vite æ„å»ºé…ç½®

- `src/config/index.ts` - å‰ç«¯é…ç½®

**åç«¯ï¼š**

---

```bash

cd kai-server## ğŸ› ï¸ å¼€å‘è¯´æ˜



# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ### å‰ç«¯æŠ€æœ¯æ ˆ

python -m venv .venv

source .venv/bin/activate  # Windows: .venv\Scripts\activate- Vue 3 + Composition API

- Vite 5.x

# å®‰è£…ä¾èµ–- Naive UI ç»„ä»¶åº“

pip install -r requirements.txt- Tauri 2.0 (æ¡Œé¢åº”ç”¨)

- Pinia çŠ¶æ€ç®¡ç†

# å¯åŠ¨æœåŠ¡

python app.py### åç«¯æŠ€æœ¯æ ˆ

```

- Python 3.11+

**å‰ç«¯ï¼š**- FastAPI

- SQLAlchemy + SQLite

```bash- LangChain + Ollama

cd kai-client- ChromaDB (å‘é‡æ•°æ®åº“)



# å®‰è£…ä¾èµ–### åº”ç”¨æµç¨‹èŠ‚ç‚¹ (å¾…å¼€å‘)

npm install

> å‚è€ƒ Dify/FastGPT çš„å·¥ä½œæµè®¾è®¡

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨

npm run dev**Start èŠ‚ç‚¹è®¾è®¡è€ƒè™‘ï¼š**

- è¾“å…¥å­—æ®µå¯è‡ªå®šä¹‰

# æ„å»ºç”Ÿäº§ç‰ˆæœ¬- è¾“å‡ºå­—æ®µä¿ç•™æ ‡å‡†æ ¼å¼

npm run build- å…¨å±€å˜é‡å¯åœ¨ç³»ç»Ÿé…ç½®ä¸­è®¾ç½®

```

---

### æŠ€æœ¯æ ˆ

## ğŸ“ æ›´æ–°æ—¥å¿—

**å‰ç«¯ï¼š**

- Vue 3 + Composition API- ç®€åŒ– LLM é…ç½®ï¼Œä»…æ”¯æŒ Ollama æœ¬åœ°æ¨¡å‹

- Vite 5.x- ç§»é™¤ç¬¬ä¸‰æ–¹ API ä¾èµ– (OpenAI, DeepSeek, Moonshot ç­‰)

- Naive UI- æ·»åŠ ä¸€é”®å¯åŠ¨è„šæœ¬

- Pinia- ä¼˜åŒ–åµŒå…¥æ¨¡å‹åŠ è½½

- Tauri 2.0 (å¯é€‰ï¼Œç”¨äºæ¡Œé¢åº”ç”¨)

---

**åç«¯ï¼š**

- Python 3.11+## ğŸ“„ License

- FastAPI

- SQLAlchemy + SQLiteMIT License

- LangChain

- ChromaDB---



---## ğŸ™ è‡´è°¢



## ğŸ“ å¸¸è§é—®é¢˜- [Ollama](https://ollama.ai/) - æœ¬åœ° LLM æœåŠ¡

- [M3E](https://huggingface.co/moka-ai/m3e-small) - ä¸­æ–‡åµŒå…¥æ¨¡å‹

<details>- [LangChain](https://langchain.com/) - LLM åº”ç”¨æ¡†æ¶

<summary><b>Q: å¯åŠ¨æ—¶æç¤ºç«¯å£è¢«å ç”¨ï¼Ÿ</b></summary>- [Naive UI](https://naiveui.com/) - Vue 3 ç»„ä»¶åº“

- [Tauri](https://tauri.app/) - æ¡Œé¢åº”ç”¨æ¡†æ¶

```bash
# æŸ¥çœ‹å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :6088
lsof -i :11420

# ç»ˆæ­¢è¿›ç¨‹
kill -9 <PID>
```
</details>

<details>
<summary><b>Q: Ollama æ¨¡å‹ä¸‹è½½æ…¢ï¼Ÿ</b></summary>

å¯ä»¥è®¾ç½®ä»£ç†æˆ–ä½¿ç”¨é•œåƒï¼š
```bash
export OLLAMA_HOST=https://your-mirror.com
ollama pull qwen2
```
</details>

<details>
<summary><b>Q: M3E æ¨¡å‹åŠ è½½å¤±è´¥ï¼Ÿ</b></summary>

æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæ¨¡å‹ä¼šä» HuggingFace è‡ªåŠ¨ä¸‹è½½ã€‚å¦‚æœç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½åˆ° `kai-server/resources/model/m3e/` ç›®å½•ã€‚
</details>

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

---

## ğŸ“„ License

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE) å¼€æºåè®®ã€‚

---

## ğŸ™ è‡´è°¢

- [Ollama](https://ollama.ai/) - æœ¬åœ° LLM æœåŠ¡
- [M3E](https://huggingface.co/moka-ai/m3e-small) - ä¸­æ–‡åµŒå…¥æ¨¡å‹
- [LangChain](https://langchain.com/) - LLM åº”ç”¨æ¡†æ¶
- [Naive UI](https://naiveui.com/) - Vue 3 ç»„ä»¶åº“
- [FastAPI](https://fastapi.tiangolo.com/) - Python Web æ¡†æ¶
